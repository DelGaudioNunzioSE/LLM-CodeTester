# Introduction
This project is part of a larger initiative that required verifying whether code generated by LLMs was correctly produced.

The project was originally based on the [KodCode](https://kodcode-ai.github.io/) framework, but progressively underwent numerous changes, eventually resulting in a completely different code organization.

# Overview
The notebook `main.ipynb` is designed to help users quickly understand how to use this framework.



### ‚ùì Why does this framework exist?

This project aims to provide a small but meaningful contribution to the research on LLM-generated code detection.
It addresses a common issue: many datasets include LLM-generated code samples without verifying their actual correctness.
This framework focuses on evaluating whether those code samples are functionally correct.

### üéØ Objectives

1. **To evaluate whether a detection method is biased** ‚Äî for example, whether it is more likely to classify LLM-generated code as such when it contains functional errors.
2. **To support the training of ML-based detectors** on datasets containing only correct, working code samples.

### Possible other use
- This framework can also be used to test multiple code samples (including human-written code) on a set of well-known programming problems

## ‚ö†Ô∏è Limitations

1. **Currently supports only Python code.**

2. **Currently only files that contain a function or a class with a single method are currently testable.**

   * This is due to a code-cleaning step applied before passing the code to the LLM.The cleaning process is intentionally strict to ensure that the code given to the LLM closely matches the description provided in the prompt.

2. **The most thoroughly tested evaluation strategy requires a correct reference solution** to the same problem.

   * This is due to generating test code from scratch using an LLM typically requires large models. In contrast, generating only input variations can be done using much smaller models.
